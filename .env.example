# ContextEngine Configuration
# Copy this to .env and fill in your values

# ─── LLM Backend ───────────────────────────────────────────
# Choose: "openrouter" (cloud) or "ollama" (local)
LLM_BACKEND=openrouter

# OpenRouter (required if LLM_BACKEND=openrouter)
# Get your key at https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-v1-your-key-here

# Ollama (required if LLM_BACKEND=ollama)
# Install Ollama: https://ollama.ai
# Then: ollama pull llama3.2:3b && ollama pull llama3.1:8b
# OLLAMA_URL=http://host.docker.internal:11434
# OLLAMA_MODEL_LIGHT=llama3.2:3b
# OLLAMA_MODEL_HEAVY=llama3.1:8b

# ─── Optional ──────────────────────────────────────────────
# Port to expose (default: 9040)
# CE_PORT=9040

# Start in learning mode (logs only, no context updates)
# Recommended for first few sessions to build baseline
# LEARNING_MODE=true

# ContextEngine Configuration
# Copy this to .env and fill in your values
# Or skip this entirely and configure via the dashboard Settings tab.

# ─── LLM Provider ──────────────────────────────────────────
# Any OpenAI-compatible API works. Pick your provider:
#
# OpenRouter (recommended, 200+ models):
#   LLM_BASE_URL=https://openrouter.ai/api/v1
#   LLM_API_KEY=sk-or-v1-your-key-here
#   Get key: https://openrouter.ai/keys
#
# OpenAI direct:
#   LLM_BASE_URL=https://api.openai.com/v1
#   LLM_API_KEY=sk-your-key-here
#
# Ollama (local, free):
#   LLM_BASE_URL=http://host.docker.internal:11434/v1
#   LLM_API_KEY=
#   Install: https://ollama.com/download
#   Then: ollama pull llama3.2:3b && ollama pull llama3.1:8b
#
# Groq (fast, free tier):
#   LLM_BASE_URL=https://api.groq.com/openai/v1
#   LLM_API_KEY=gsk_your-key-here
#
# Together AI:
#   LLM_BASE_URL=https://api.together.xyz/v1
#   LLM_API_KEY=your-key-here

LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_API_KEY=sk-or-v1-your-key-here

# Models — "fast" handles extraction/summaries, "smart" handles triage/compression
# These are good defaults for OpenRouter. Change to match your provider.
LLM_MODEL_FAST=anthropic/claude-haiku-4.5
LLM_MODEL_SMART=anthropic/claude-haiku-4.5

# ─── File Watcher (optional) ──────────────────────────────
# Mount a directory to /watch in the container, then list subdirs here.
# The watcher auto-commits changes to git and feeds them into the context pipeline.
# WATCH_DIRS=/watch/stacks,/watch/projects
# WATCH_DEBOUNCE_SECONDS=10
# WATCH_TRANSCRIPT_DIR=/watch/transcripts

# ─── Telegram Notifications (optional) ────────────────────
# Get a bot token from @BotFather on Telegram.
# TELEGRAM_BOT_TOKEN=
# TELEGRAM_CHAT_ID=

# ─── General ──────────────────────────────────────────────
# CE_PORT=9040
# LEARNING_MODE=true
